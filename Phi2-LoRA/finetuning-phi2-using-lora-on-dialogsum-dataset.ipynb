{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:43:10.487547Z","iopub.status.busy":"2024-05-16T05:43:10.487159Z","iopub.status.idle":"2024-05-16T05:43:23.979908Z","shell.execute_reply":"2024-05-16T05:43:23.978798Z","shell.execute_reply.started":"2024-05-16T05:43:10.487495Z"},"trusted":true},"outputs":[],"source":["# !pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-16T05:43:36.711948Z","iopub.status.busy":"2024-05-16T05:43:36.711015Z","iopub.status.idle":"2024-05-16T05:44:18.229704Z","shell.execute_reply":"2024-05-16T05:44:18.228646Z","shell.execute_reply.started":"2024-05-16T05:43:36.711911Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-16 05:43:41.800867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-16 05:43:41.800926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-16 05:43:41.802436: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"]},{"name":"stdout","output_type":"stream","text":["Enter your token (input will not be visible):  ·····································\n","Add token as git credential? (Y/n)  Y\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    GenerationConfig\n",")\n","from tqdm import tqdm\n","from trl import SFTTrainer\n","import torch\n","import time\n","import pandas as pd\n","import numpy as np\n","from huggingface_hub import interpreter_login\n","from utils import print_gpu_utilization\n","\n","interpreter_login()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:26.646452Z","iopub.status.busy":"2024-05-16T05:44:26.645431Z","iopub.status.idle":"2024-05-16T05:44:26.651476Z","shell.execute_reply":"2024-05-16T05:44:26.650604Z","shell.execute_reply.started":"2024-05-16T05:44:26.646415Z"},"trusted":true},"outputs":[],"source":["from pynvml import *\n","\n","def print_gpu_utilization():\n","    nvmlInit()\n","    handle = nvmlDeviceGetHandleByIndex(0)\n","    info = nvmlDeviceGetMemoryInfo(handle)\n","    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:28.993939Z","iopub.status.busy":"2024-05-16T05:44:28.993548Z","iopub.status.idle":"2024-05-16T05:44:28.998617Z","shell.execute_reply":"2024-05-16T05:44:28.997627Z","shell.execute_reply.started":"2024-05-16T05:44:28.993909Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['WANDB_DISABLED']=\"true\"                 # disable Weights and Biases"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:30.625096Z","iopub.status.busy":"2024-05-16T05:44:30.624405Z","iopub.status.idle":"2024-05-16T05:44:33.586711Z","shell.execute_reply":"2024-05-16T05:44:33.585896Z","shell.execute_reply.started":"2024-05-16T05:44:30.625064Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"995be2c24e304f3d87fe73c14f902d10","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58c0f902b5f14b1e9aa7e527307c53d2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faa9996130ce40aeb355f386bd176a89","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/441k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84b97d17bb73492b8dc5556a017e017e","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/447k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d15bfd37fd3444c888e38932d2504a4","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecf92aee56d045b08582527c197f3ceb","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92e0acd7114048f9be8882a58b4863ed","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'id': 'train_0',\n"," 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n"," 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n"," 'topic': 'get a check-up'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["huggingface_dataset_name = \"neil-code/dialogsum-test\"\n","dataset = load_dataset(huggingface_dataset_name)\n","\n","dataset['train'][0]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:37.170076Z","iopub.status.busy":"2024-05-16T05:44:37.169696Z","iopub.status.idle":"2024-05-16T05:45:04.747746Z","shell.execute_reply":"2024-05-16T05:45:04.746757Z","shell.execute_reply.started":"2024-05-16T05:44:37.170034Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6394a8edd474ec3915b9085e2cf1c3f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9809078885d4e5dbed57ad0d4663cbf","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0dcf4156de5a4144818ecdb2220d86f0","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91f4926295974378b98d06f3a8dcbe1f","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7729ecd177d641508da39de48dbaf0ea","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b247c8a7e0f648a49d6edd58cd84e8f0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f216f05cc694f0e82eba834781180c8","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=False,\n","    )\n","\n","model_name='microsoft/phi-2'\n","device_map = {\"\": 0}\n","original_model = AutoModelForCausalLM.from_pretrained(model_name, \n","                                                      device_map=device_map,\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:45:45.682850Z","iopub.status.busy":"2024-05-16T05:45:45.682057Z","iopub.status.idle":"2024-05-16T05:45:47.195604Z","shell.execute_reply":"2024-05-16T05:45:47.194549Z","shell.execute_reply.started":"2024-05-16T05:45:45.682806Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9eb556b0450f463489587ea76ed4580e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a4cbf79df54cc3984fec191272b574","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"982503372ac74170ac400a7d71f22c15","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01ed35d6e37f4ffd8d5d41de7454e0db","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be651c81a47b41dcb045fb8d3429422a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b46151c41887491e8e1d068911a625d8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 2318 MB.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token\n","\n","def gen(model,p, maxlen=100, sample=True):\n","    toks = eval_tokenizer(p, return_tensors=\"pt\")\n","    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n","    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n","\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:45:50.802639Z","iopub.status.busy":"2024-05-16T05:45:50.801976Z","iopub.status.idle":"2024-05-16T05:45:55.774924Z","shell.execute_reply":"2024-05-16T05:45:55.773986Z","shell.execute_reply.started":"2024-05-16T05:45:50.802605Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n","\n","CPU times: user 4.18 s, sys: 222 ms, total: 4.4 s\n","Wall time: 4.97 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","seed = 42\n","set_seed(seed)\n","\n","index = 10\n","\n","prompt = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n","res = gen(original_model,formatted_prompt,100,)\n","#print(res[0])\n","output = res[0].split('Output:\\n')[1]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{formatted_prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:02.916825Z","iopub.status.busy":"2024-05-16T05:46:02.916454Z","iopub.status.idle":"2024-05-16T05:46:02.929291Z","shell.execute_reply":"2024-05-16T05:46:02.928242Z","shell.execute_reply.started":"2024-05-16T05:46:02.916794Z"},"trusted":true},"outputs":[],"source":["def create_prompt_formats(sample):\n","    \"\"\"\n","    Format various fields of the sample ('instruction','output')\n","    Then concatenate them using two newline characters \n","    :param sample: Sample dictionnary\n","    \"\"\"\n","    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n","    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n","    RESPONSE_KEY = \"### Output:\"\n","    END_KEY = \"### End\"\n","    \n","    blurb = f\"\\n{INTRO_BLURB}\"\n","    instruction = f\"{INSTRUCTION_KEY}\"\n","    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n","    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n","    end = f\"{END_KEY}\"\n","    \n","    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n","\n","    formatted_prompt = \"\\n\\n\".join(parts)\n","    sample[\"text\"] = formatted_prompt\n","\n","    return sample\n","\n","# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n","def get_max_length(model):\n","    conf = model.config\n","    max_length = None\n","    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n","        max_length = getattr(model.config, length_setting, None)\n","        if max_length:\n","            print(f\"Found max lenth: {max_length}\")\n","            break\n","    if not max_length:\n","        max_length = 1024\n","        print(f\"Using default max length: {max_length}\")\n","    return max_length\n","\n","\n","def preprocess_batch(batch, tokenizer, max_length):\n","    \"\"\"\n","    Tokenizing a batch\n","    \"\"\"\n","    return tokenizer(\n","        batch[\"text\"],\n","        max_length=max_length,\n","        truncation=True,\n","    )\n","\n","from functools import partial\n","\n","# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n","def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n","    \"\"\"Format & tokenize it so it is ready for training\n","    :param tokenizer (AutoTokenizer): Model Tokenizer\n","    :param max_length (int): Maximum number of tokens to emit from tokenizer\n","    \"\"\"\n","    \n","    # Add prompt to each sample\n","    print(\"Preprocessing dataset...\")\n","    dataset = dataset.map(create_prompt_formats)#, batched=True)\n","    \n","    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n","    dataset = dataset.map(\n","        _preprocessing_function,\n","        batched=True,\n","        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n","    )\n","\n","    # Filter out samples that have input_ids exceeding max_length\n","    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n","    \n","    # Shuffle dataset\n","    dataset = dataset.shuffle(seed=seed)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:07.143936Z","iopub.status.busy":"2024-05-16T05:46:07.143081Z","iopub.status.idle":"2024-05-16T05:46:19.755848Z","shell.execute_reply":"2024-05-16T05:46:19.754914Z","shell.execute_reply.started":"2024-05-16T05:46:07.143901Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found max lenth: 2048\n","2048\n","Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"229416da9ccc453aa52f0a0a87920e72","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d61c6206e6a4832912e0f5516e137d6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1037c87fb9b4f95a233c433153365b9","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05d2799ea92d48df90892296afd26c43","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64dad657808842a0bb0bc9eba79070a4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5630ef6e034b4732b09845b748bc1297","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Shapes of the datasets:\n","Training: (1999, 3)\n","Validation: (499, 3)\n","Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 1999\n","})\n","GPU memory occupied: 2596 MB.\n"]}],"source":["# ## Pre-process dataset\n","max_length = get_max_length(original_model)\n","print(max_length)\n","\n","train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n","eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n","\n","print(f\"Shapes of the datasets:\")\n","print(f\"Training: {train_dataset.shape}\")\n","print(f\"Validation: {eval_dataset.shape}\")\n","print(train_dataset)\n","\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:25.970310Z","iopub.status.busy":"2024-05-16T05:46:25.969914Z","iopub.status.idle":"2024-05-16T05:46:25.984864Z","shell.execute_reply":"2024-05-16T05:46:25.983722Z","shell.execute_reply.started":"2024-05-16T05:46:25.970276Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 262364160\n","all model parameters: 1521392640\n","percentage of trainable model parameters: 17.24% \n","\n","PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")\n"]}],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","\n","print(print_number_of_trainable_model_parameters(original_model),\"\\n\")\n","print(original_model)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:29.022906Z","iopub.status.busy":"2024-05-16T05:46:29.022251Z","iopub.status.idle":"2024-05-16T05:46:29.491374Z","shell.execute_reply":"2024-05-16T05:46:29.490394Z","shell.execute_reply.started":"2024-05-16T05:46:29.022873Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 20971520\n","all model parameters: 1542364160\n","percentage of trainable model parameters: 1.36%\n","PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): PhiForCausalLM(\n","      (model): PhiModel(\n","        (embed_tokens): Embedding(51200, 2560)\n","        (embed_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-31): 32 x PhiDecoderLayer(\n","            (self_attn): PhiSdpaAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (dense): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (rotary_emb): PhiRotaryEmbedding()\n","            )\n","            (mlp): PhiMLP(\n","              (activation_fn): NewGELUActivation()\n","              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","            )\n","            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","config = LoraConfig(\n","    r=32, #Rank\n","    lora_alpha=32,\n","    target_modules=[\n","        'q_proj',\n","        'k_proj',\n","        'v_proj',\n","        'dense'\n","    ],\n","    bias=\"none\",\n","    lora_dropout=0.05,  # Conventional\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n","original_model.gradient_checkpointing_enable()\n","\n","# 2 - Using the prepare_model_for_kbit_training method from PEFT\n","original_model = prepare_model_for_kbit_training(original_model)\n","\n","peft_model = get_peft_model(original_model, config)\n","\n","print(print_number_of_trainable_model_parameters(peft_model))\n","print(peft_model)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:34.633727Z","iopub.status.busy":"2024-05-16T05:46:34.633346Z","iopub.status.idle":"2024-05-16T05:46:34.675957Z","shell.execute_reply":"2024-05-16T05:46:34.674986Z","shell.execute_reply.started":"2024-05-16T05:46:34.633698Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["output_dir = './peft-dialogue-summary-training/final-checkpoint'\n","import transformers\n","\n","peft_training_args = TrainingArguments(\n","    output_dir = output_dir,\n","    warmup_steps=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    max_steps=1000,\n","    learning_rate=2e-4,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=25,\n","    logging_dir=\"./logs\",\n","    save_strategy=\"steps\",\n","    save_steps=25,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=25,\n","    do_eval=True,\n","    gradient_checkpointing=True,\n","    report_to=\"none\",\n","    overwrite_output_dir = 'True',\n","    group_by_length=True,\n",")\n","\n","peft_model.config.use_cache = False\n","\n","peft_trainer = transformers.Trainer(\n","    model=peft_model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    args=peft_training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:41.124186Z","iopub.status.busy":"2024-05-16T05:46:41.123812Z","iopub.status.idle":"2024-05-16T09:49:43.257947Z","shell.execute_reply":"2024-05-16T09:49:43.256620Z","shell.execute_reply.started":"2024-05-16T05:46:41.124158Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 4:02:42, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.628200</td>\n","      <td>1.381187</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.204600</td>\n","      <td>1.397275</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.426900</td>\n","      <td>1.348176</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.150100</td>\n","      <td>1.380293</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.469600</td>\n","      <td>1.338985</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.191100</td>\n","      <td>1.346107</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.423600</td>\n","      <td>1.334573</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.193900</td>\n","      <td>1.338646</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.450000</td>\n","      <td>1.331974</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.193200</td>\n","      <td>1.339612</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.463100</td>\n","      <td>1.328855</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.150400</td>\n","      <td>1.333416</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.348800</td>\n","      <td>1.326381</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.131500</td>\n","      <td>1.332278</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.433200</td>\n","      <td>1.325377</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.161500</td>\n","      <td>1.328729</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>1.385300</td>\n","      <td>1.324337</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.152100</td>\n","      <td>1.326642</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.403500</td>\n","      <td>1.323025</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.141800</td>\n","      <td>1.324953</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.407200</td>\n","      <td>1.323117</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.138400</td>\n","      <td>1.327563</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.359700</td>\n","      <td>1.323132</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.128500</td>\n","      <td>1.327374</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>1.373200</td>\n","      <td>1.322586</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.134000</td>\n","      <td>1.325992</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.378700</td>\n","      <td>1.321076</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.096800</td>\n","      <td>1.323717</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.403500</td>\n","      <td>1.320189</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.090100</td>\n","      <td>1.324304</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>1.368900</td>\n","      <td>1.319462</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.096600</td>\n","      <td>1.323311</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>1.369200</td>\n","      <td>1.320460</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.104000</td>\n","      <td>1.322982</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>1.345800</td>\n","      <td>1.320821</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.092300</td>\n","      <td>1.322499</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>1.385100</td>\n","      <td>1.321084</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.072300</td>\n","      <td>1.322091</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>1.349700</td>\n","      <td>1.321413</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.095700</td>\n","      <td>1.321211</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 14858 MB.\n"]}],"source":["print(peft_training_args.device)\n","peft_trainer.train()\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:29:07.856491Z","iopub.status.busy":"2024-05-16T10:29:07.855775Z","iopub.status.idle":"2024-05-16T10:29:08.081682Z","shell.execute_reply":"2024-05-16T10:29:08.080653Z","shell.execute_reply.started":"2024-05-16T10:29:07.856454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 3584 MB.\n"]}],"source":["del original_model\n","del peft_trainer\n","torch.cuda.empty_cache()\n","print_gpu_utilization()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:31:38.820948Z","iopub.status.busy":"2024-05-16T10:31:38.820053Z","iopub.status.idle":"2024-05-16T10:31:42.609643Z","shell.execute_reply":"2024-05-16T10:31:42.608761Z","shell.execute_reply.started":"2024-05-16T10:31:38.820913Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81ad8cfd9bb4467aa8de04187dfb5fa1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["base_model_id = \"microsoft/phi-2\"\n","base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:31:43.511733Z","iopub.status.busy":"2024-05-16T10:31:43.511116Z","iopub.status.idle":"2024-05-16T10:31:44.397958Z","shell.execute_reply":"2024-05-16T10:31:44.396926Z","shell.execute_reply.started":"2024-05-16T10:31:43.511702Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token\n","\n","from peft import PeftModel\n","\n","ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:31:45.346678Z","iopub.status.busy":"2024-05-16T10:31:45.346289Z","iopub.status.idle":"2024-05-16T10:31:54.494016Z","shell.execute_reply":"2024-05-16T10:31:54.493090Z","shell.execute_reply.started":"2024-05-16T10:31:45.346648Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","PEFT MODEL:\n","#Person1# gives Brian a birthday gift and invites him to dance. Brian thinks #Person1# looks great and they decide to celebrate together.\n","\n","### End of Output.\n","\n","### Output 2.\n","\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#:\n","CPU times: user 9.11 s, sys: 23.1 ms, total: 9.13 s\n","Wall time: 9.14 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","set_seed(seed)\n","\n","index = 10\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","\n","peft_model_res = gen(ft_model,prompt,100,)\n","peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n","#print(peft_model_output)\n","prefix, success, result = peft_model_output.partition('#End')\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'PEFT MODEL:\\n{prefix}')"]},{"cell_type":"markdown","metadata":{},"source":["# ROUGE Metric"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:33:31.717396Z","iopub.status.busy":"2024-05-16T10:33:31.716686Z","iopub.status.idle":"2024-05-16T10:33:44.022673Z","shell.execute_reply":"2024-05-16T10:33:44.021426Z","shell.execute_reply.started":"2024-05-16T10:33:31.717358Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n"]}],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:33:44.026196Z","iopub.status.busy":"2024-05-16T10:33:44.025330Z","iopub.status.idle":"2024-05-16T10:36:07.266578Z","shell.execute_reply":"2024-05-16T10:36:07.265622Z","shell.execute_reply.started":"2024-05-16T10:33:44.026152Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"986389fc6ac3457c98dd2bedb4c73132","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>peft_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. They have...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. They have...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha tells Kate that Masha and Hero are getti...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>Person1 and Person2 are at a party, and Person...</td>\n","      <td>#Person1# gives Brian a birthday gift and invi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0  Person 1: Ms. Dawson, I need you to take a dic...   \n","1  Person 1: Ms. Dawson, I need you to take a dic...   \n","2  Person 1: Ms. Dawson, I need you to take a dic...   \n","3  Person1 and Person2 are discussing the traffic...   \n","4  Person1 and Person2 are discussing the traffic...   \n","5  Person1 and Person2 are discussing the traffic...   \n","6  Kate informed that Masha and Hero are getting ...   \n","7  Kate informed that Masha and Hero are getting ...   \n","8  Kate informed that Masha and Hero are getting ...   \n","9  Person1 and Person2 are at a party, and Person...   \n","\n","                                peft_model_summaries  \n","0  #Person1# asks Ms. Dawson to take a dictation ...  \n","1  #Person1# asks Ms. Dawson to take a dictation ...  \n","2  #Person1# asks Ms. Dawson to take a dictation ...  \n","3  #Person2# got stuck in traffic again and #Pers...  \n","4  #Person2# got stuck in traffic again and #Pers...  \n","5  #Person2# got stuck in traffic again and #Pers...  \n","6  Masha and Hero are getting divorced. They have...  \n","7  Masha and Hero are getting divorced. They have...  \n","8  Masha tells Kate that Masha and Hero are getti...  \n","9  #Person1# gives Brian a birthday gift and invi...  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)\n","\n","import pandas as pd\n","\n","dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","peft_model_summaries = []\n","\n","for idx, dialogue in enumerate(dialogues):\n","    human_baseline_text_output = human_baseline_summaries[idx]\n","    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","    \n","    original_model_res = gen(original_model,prompt,100,)\n","    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n","    \n","    peft_model_res = gen(ft_model,prompt,100,)\n","    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n","    #print(peft_model_output)\n","    peft_model_text_output, success, result = peft_model_output.partition('#End')\n","    \n","\n","    original_model_summaries.append(original_model_text_output)\n","    peft_model_summaries.append(peft_model_text_output)\n","\n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n","df"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:36:07.268097Z","iopub.status.busy":"2024-05-16T10:36:07.267801Z","iopub.status.idle":"2024-05-16T10:36:09.079561Z","shell.execute_reply":"2024-05-16T10:36:09.078317Z","shell.execute_reply.started":"2024-05-16T10:36:07.268071Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e079e26ef4f64cee883288119d2e08bf","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ORIGINAL MODEL:\n","{'rouge1': 0.2808829786653489, 'rouge2': 0.10594067089744544, 'rougeL': 0.20164900056627, 'rougeLsum': 0.2125258194438865}\n","PEFT MODEL:\n","{'rouge1': 0.31440242150611264, 'rouge2': 0.12777019949969665, 'rougeL': 0.24896680231457047, 'rougeLsum': 0.2578809037890624}\n"]}],"source":["import evaluate\n","\n","rouge = evaluate.load('rouge')\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_summaries,\n","    references=human_baseline_summaries[0:len(peft_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('PEFT MODEL:')\n","print(peft_model_results)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:36:09.083248Z","iopub.status.busy":"2024-05-16T10:36:09.082332Z","iopub.status.idle":"2024-05-16T10:36:09.090509Z","shell.execute_reply":"2024-05-16T10:36:09.089383Z","shell.execute_reply.started":"2024-05-16T10:36:09.083205Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n","rouge1: 3.35%\n","rouge2: 2.18%\n","rougeL: 4.73%\n","rougeLsum: 4.54%\n"]}],"source":["print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","\n","improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(peft_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Evaluation on Basic Prompts"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T11:05:53.305157Z","iopub.status.busy":"2024-05-16T11:05:53.304703Z","iopub.status.idle":"2024-05-16T11:05:55.886562Z","shell.execute_reply":"2024-05-16T11:05:55.885616Z","shell.execute_reply.started":"2024-05-16T11:05:53.305121Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Person1 and Person2 are at a party. Person1 compliments Person2 on their party and appearance. Person1 asks Person2 to dance. Person1 and Person2 have a drink together to celebrate.\n","\n"]}],"source":["dialogue = \"#Person1#: Happy Birthday, this is for you, Brian. Person2: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time. Person1: Brian, may I have a pleasure to have a dance with you? Person2: Ok. Person1: This is really wonderful party. Person2: Yes, you are always popular with everyone. and you look very pretty today. Person1: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel. Person2: You look great, you are absolutely glowing. Person1: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\"\n","prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","\n","original_model_res = gen(original_model,prompt,100,)\n","original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n","\n","print(original_model_text_output)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
