{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:43:10.487547Z","iopub.status.busy":"2024-05-16T05:43:10.487159Z","iopub.status.idle":"2024-05-16T05:43:23.979908Z","shell.execute_reply":"2024-05-16T05:43:23.978798Z","shell.execute_reply.started":"2024-05-16T05:43:10.487495Z"},"trusted":true},"outputs":[],"source":["# !pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-16T05:43:36.711948Z","iopub.status.busy":"2024-05-16T05:43:36.711015Z","iopub.status.idle":"2024-05-16T05:44:18.229704Z","shell.execute_reply":"2024-05-16T05:44:18.228646Z","shell.execute_reply.started":"2024-05-16T05:43:36.711911Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-16 05:43:41.800867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-16 05:43:41.800926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-16 05:43:41.802436: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"]},{"name":"stdout","output_type":"stream","text":["Enter your token (input will not be visible):  ·····································\n","Add token as git credential? (Y/n)  Y\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    GenerationConfig\n",")\n","from tqdm import tqdm\n","from trl import SFTTrainer\n","import torch\n","import time\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","from huggingface_hub import interpreter_login\n","from utils.functions import print_gpu_utilization, create_prompt_formats, get_max_length, preprocess_batch, preprocess_dataset\n","from functools import partial\n","\n","interpreter_login()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:28.993939Z","iopub.status.busy":"2024-05-16T05:44:28.993548Z","iopub.status.idle":"2024-05-16T05:44:28.998617Z","shell.execute_reply":"2024-05-16T05:44:28.997627Z","shell.execute_reply.started":"2024-05-16T05:44:28.993909Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['WANDB_DISABLED']=\"true\"                 # disable Weights and Biases"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:30.625096Z","iopub.status.busy":"2024-05-16T05:44:30.624405Z","iopub.status.idle":"2024-05-16T05:44:33.586711Z","shell.execute_reply":"2024-05-16T05:44:33.585896Z","shell.execute_reply.started":"2024-05-16T05:44:30.625064Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"995be2c24e304f3d87fe73c14f902d10","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58c0f902b5f14b1e9aa7e527307c53d2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faa9996130ce40aeb355f386bd176a89","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/441k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84b97d17bb73492b8dc5556a017e017e","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/447k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d15bfd37fd3444c888e38932d2504a4","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecf92aee56d045b08582527c197f3ceb","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92e0acd7114048f9be8882a58b4863ed","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'id': 'train_0',\n"," 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n"," 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n"," 'topic': 'get a check-up'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["huggingface_dataset_name = \"neil-code/dialogsum-test\"\n","dataset = load_dataset(huggingface_dataset_name)\n","\n","dataset['train'][0]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:44:37.170076Z","iopub.status.busy":"2024-05-16T05:44:37.169696Z","iopub.status.idle":"2024-05-16T05:45:04.747746Z","shell.execute_reply":"2024-05-16T05:45:04.746757Z","shell.execute_reply.started":"2024-05-16T05:44:37.170034Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6394a8edd474ec3915b9085e2cf1c3f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9809078885d4e5dbed57ad0d4663cbf","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0dcf4156de5a4144818ecdb2220d86f0","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91f4926295974378b98d06f3a8dcbe1f","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7729ecd177d641508da39de48dbaf0ea","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b247c8a7e0f648a49d6edd58cd84e8f0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f216f05cc694f0e82eba834781180c8","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=False,\n","    )\n","\n","model_name='microsoft/phi-2'\n","device_map = {\"\": 0}\n","original_model = AutoModelForCausalLM.from_pretrained(model_name, \n","                                                      device_map=device_map,\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:45:45.682850Z","iopub.status.busy":"2024-05-16T05:45:45.682057Z","iopub.status.idle":"2024-05-16T05:45:47.195604Z","shell.execute_reply":"2024-05-16T05:45:47.194549Z","shell.execute_reply.started":"2024-05-16T05:45:45.682806Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9eb556b0450f463489587ea76ed4580e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a4cbf79df54cc3984fec191272b574","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"982503372ac74170ac400a7d71f22c15","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01ed35d6e37f4ffd8d5d41de7454e0db","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be651c81a47b41dcb045fb8d3429422a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b46151c41887491e8e1d068911a625d8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 2318 MB.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token\n","\n","def gen(model,p, maxlen=100, sample=True):\n","    toks = eval_tokenizer(p, return_tensors=\"pt\")\n","    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n","    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n","\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:45:50.802639Z","iopub.status.busy":"2024-05-16T05:45:50.801976Z","iopub.status.idle":"2024-05-16T05:45:55.774924Z","shell.execute_reply":"2024-05-16T05:45:55.773986Z","shell.execute_reply.started":"2024-05-16T05:45:50.802605Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n","\n","CPU times: user 4.18 s, sys: 222 ms, total: 4.4 s\n","Wall time: 4.97 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","seed = 42\n","set_seed(seed)\n","\n","index = 10\n","\n","prompt = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n","res = gen(original_model,formatted_prompt,100,)\n","#print(res[0])\n","output = res[0].split('Output:\\n')[1]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{formatted_prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:07.143936Z","iopub.status.busy":"2024-05-16T05:46:07.143081Z","iopub.status.idle":"2024-05-16T05:46:19.755848Z","shell.execute_reply":"2024-05-16T05:46:19.754914Z","shell.execute_reply.started":"2024-05-16T05:46:07.143901Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found max lenth: 2048\n","2048\n","Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"229416da9ccc453aa52f0a0a87920e72","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d61c6206e6a4832912e0f5516e137d6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1037c87fb9b4f95a233c433153365b9","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05d2799ea92d48df90892296afd26c43","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64dad657808842a0bb0bc9eba79070a4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5630ef6e034b4732b09845b748bc1297","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Shapes of the datasets:\n","Training: (1999, 3)\n","Validation: (499, 3)\n","Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 1999\n","})\n","GPU memory occupied: 2596 MB.\n"]}],"source":["# ## Pre-process dataset\n","max_length = get_max_length(original_model)\n","print(max_length)\n","\n","train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n","eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n","\n","print(f\"Shapes of the datasets:\")\n","print(f\"Training: {train_dataset.shape}\")\n","print(f\"Validation: {eval_dataset.shape}\")\n","print(train_dataset)\n","\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:25.970310Z","iopub.status.busy":"2024-05-16T05:46:25.969914Z","iopub.status.idle":"2024-05-16T05:46:25.984864Z","shell.execute_reply":"2024-05-16T05:46:25.983722Z","shell.execute_reply.started":"2024-05-16T05:46:25.970276Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 262364160\n","all model parameters: 1521392640\n","percentage of trainable model parameters: 17.24% \n","\n","PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")\n"]}],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","\n","print(print_number_of_trainable_model_parameters(original_model),\"\\n\")\n","print(original_model)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:29.022906Z","iopub.status.busy":"2024-05-16T05:46:29.022251Z","iopub.status.idle":"2024-05-16T05:46:29.491374Z","shell.execute_reply":"2024-05-16T05:46:29.490394Z","shell.execute_reply.started":"2024-05-16T05:46:29.022873Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 20971520\n","all model parameters: 1542364160\n","percentage of trainable model parameters: 1.36%\n","PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): PhiForCausalLM(\n","      (model): PhiModel(\n","        (embed_tokens): Embedding(51200, 2560)\n","        (embed_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-31): 32 x PhiDecoderLayer(\n","            (self_attn): PhiSdpaAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (dense): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (rotary_emb): PhiRotaryEmbedding()\n","            )\n","            (mlp): PhiMLP(\n","              (activation_fn): NewGELUActivation()\n","              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","            )\n","            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","config = LoraConfig(\n","    r=32, #Rank\n","    lora_alpha=32,\n","    target_modules=[\n","        'q_proj',\n","        'k_proj',\n","        'v_proj',\n","        'dense'\n","    ],\n","    bias=\"none\",\n","    lora_dropout=0.05,  # Conventional\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n","original_model.gradient_checkpointing_enable()\n","\n","# 2 - Using the prepare_model_for_kbit_training method from PEFT\n","original_model = prepare_model_for_kbit_training(original_model)\n","\n","peft_model = get_peft_model(original_model, config)\n","\n","print(print_number_of_trainable_model_parameters(peft_model))\n","print(peft_model)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:34.633727Z","iopub.status.busy":"2024-05-16T05:46:34.633346Z","iopub.status.idle":"2024-05-16T05:46:34.675957Z","shell.execute_reply":"2024-05-16T05:46:34.674986Z","shell.execute_reply.started":"2024-05-16T05:46:34.633698Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["output_dir = './peft-dialogue-summary-training/final-checkpoint'\n","import transformers\n","\n","peft_training_args = TrainingArguments(\n","    output_dir = output_dir,\n","    warmup_steps=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    max_steps=1000,\n","    learning_rate=2e-4,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=25,\n","    logging_dir=\"./logs\",\n","    save_strategy=\"steps\",\n","    save_steps=25,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=25,\n","    do_eval=True,\n","    gradient_checkpointing=True,\n","    report_to=\"none\",\n","    overwrite_output_dir = 'True',\n","    group_by_length=True,\n",")\n","\n","peft_model.config.use_cache = False\n","\n","peft_trainer = transformers.Trainer(\n","    model=peft_model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    args=peft_training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T05:46:41.124186Z","iopub.status.busy":"2024-05-16T05:46:41.123812Z","iopub.status.idle":"2024-05-16T09:49:43.257947Z","shell.execute_reply":"2024-05-16T09:49:43.256620Z","shell.execute_reply.started":"2024-05-16T05:46:41.124158Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 4:02:42, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.628200</td>\n","      <td>1.381187</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.204600</td>\n","      <td>1.397275</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.426900</td>\n","      <td>1.348176</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.150100</td>\n","      <td>1.380293</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.469600</td>\n","      <td>1.338985</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.191100</td>\n","      <td>1.346107</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.423600</td>\n","      <td>1.334573</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.193900</td>\n","      <td>1.338646</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.450000</td>\n","      <td>1.331974</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.193200</td>\n","      <td>1.339612</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.463100</td>\n","      <td>1.328855</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.150400</td>\n","      <td>1.333416</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.348800</td>\n","      <td>1.326381</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.131500</td>\n","      <td>1.332278</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.433200</td>\n","      <td>1.325377</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.161500</td>\n","      <td>1.328729</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>1.385300</td>\n","      <td>1.324337</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.152100</td>\n","      <td>1.326642</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.403500</td>\n","      <td>1.323025</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.141800</td>\n","      <td>1.324953</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.407200</td>\n","      <td>1.323117</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.138400</td>\n","      <td>1.327563</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.359700</td>\n","      <td>1.323132</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.128500</td>\n","      <td>1.327374</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>1.373200</td>\n","      <td>1.322586</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.134000</td>\n","      <td>1.325992</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.378700</td>\n","      <td>1.321076</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.096800</td>\n","      <td>1.323717</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.403500</td>\n","      <td>1.320189</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.090100</td>\n","      <td>1.324304</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>1.368900</td>\n","      <td>1.319462</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.096600</td>\n","      <td>1.323311</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>1.369200</td>\n","      <td>1.320460</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.104000</td>\n","      <td>1.322982</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>1.345800</td>\n","      <td>1.320821</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.092300</td>\n","      <td>1.322499</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>1.385100</td>\n","      <td>1.321084</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.072300</td>\n","      <td>1.322091</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>1.349700</td>\n","      <td>1.321413</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.095700</td>\n","      <td>1.321211</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 14858 MB.\n"]}],"source":["print(peft_training_args.device)\n","peft_trainer.train()\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:29:07.856491Z","iopub.status.busy":"2024-05-16T10:29:07.855775Z","iopub.status.idle":"2024-05-16T10:29:08.081682Z","shell.execute_reply":"2024-05-16T10:29:08.080653Z","shell.execute_reply.started":"2024-05-16T10:29:07.856454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 3584 MB.\n"]}],"source":["del original_model\n","del peft_trainer\n","torch.cuda.empty_cache()\n","print_gpu_utilization()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:31:38.820948Z","iopub.status.busy":"2024-05-16T10:31:38.820053Z","iopub.status.idle":"2024-05-16T10:31:42.609643Z","shell.execute_reply":"2024-05-16T10:31:42.608761Z","shell.execute_reply.started":"2024-05-16T10:31:38.820913Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81ad8cfd9bb4467aa8de04187dfb5fa1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["base_model_id = \"microsoft/phi-2\"\n","base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:31:43.511733Z","iopub.status.busy":"2024-05-16T10:31:43.511116Z","iopub.status.idle":"2024-05-16T10:31:44.397958Z","shell.execute_reply":"2024-05-16T10:31:44.396926Z","shell.execute_reply.started":"2024-05-16T10:31:43.511702Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token\n","\n","from peft import PeftModel\n","\n","ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:31:45.346678Z","iopub.status.busy":"2024-05-16T10:31:45.346289Z","iopub.status.idle":"2024-05-16T10:31:54.494016Z","shell.execute_reply":"2024-05-16T10:31:54.493090Z","shell.execute_reply.started":"2024-05-16T10:31:45.346648Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","PEFT MODEL:\n","#Person1# gives Brian a birthday gift and invites him to dance. Brian thinks #Person1# looks great and they decide to celebrate together.\n","\n","### End of Output.\n","\n","### Output 2.\n","\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#:\n","CPU times: user 9.11 s, sys: 23.1 ms, total: 9.13 s\n","Wall time: 9.14 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","set_seed(seed)\n","\n","index = 10\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","\n","peft_model_res = gen(ft_model,prompt,100,)\n","peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n","#print(peft_model_output)\n","prefix, success, result = peft_model_output.partition('#End')\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'PEFT MODEL:\\n{prefix}')"]},{"cell_type":"markdown","metadata":{},"source":["# ROUGE Metric"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:33:44.026196Z","iopub.status.busy":"2024-05-16T10:33:44.025330Z","iopub.status.idle":"2024-05-16T10:36:07.266578Z","shell.execute_reply":"2024-05-16T10:36:07.265622Z","shell.execute_reply.started":"2024-05-16T10:33:44.026152Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"986389fc6ac3457c98dd2bedb4c73132","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>peft_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. They have...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. They have...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha tells Kate that Masha and Hero are getti...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>Person1 and Person2 are at a party, and Person...</td>\n","      <td>#Person1# gives Brian a birthday gift and invi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0  Person 1: Ms. Dawson, I need you to take a dic...   \n","1  Person 1: Ms. Dawson, I need you to take a dic...   \n","2  Person 1: Ms. Dawson, I need you to take a dic...   \n","3  Person1 and Person2 are discussing the traffic...   \n","4  Person1 and Person2 are discussing the traffic...   \n","5  Person1 and Person2 are discussing the traffic...   \n","6  Kate informed that Masha and Hero are getting ...   \n","7  Kate informed that Masha and Hero are getting ...   \n","8  Kate informed that Masha and Hero are getting ...   \n","9  Person1 and Person2 are at a party, and Person...   \n","\n","                                peft_model_summaries  \n","0  #Person1# asks Ms. Dawson to take a dictation ...  \n","1  #Person1# asks Ms. Dawson to take a dictation ...  \n","2  #Person1# asks Ms. Dawson to take a dictation ...  \n","3  #Person2# got stuck in traffic again and #Pers...  \n","4  #Person2# got stuck in traffic again and #Pers...  \n","5  #Person2# got stuck in traffic again and #Pers...  \n","6  Masha and Hero are getting divorced. They have...  \n","7  Masha and Hero are getting divorced. They have...  \n","8  Masha tells Kate that Masha and Hero are getti...  \n","9  #Person1# gives Brian a birthday gift and invi...  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)\n","\n","\n","dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","peft_model_summaries = []\n","\n","for idx, dialogue in enumerate(dialogues):\n","    human_baseline_text_output = human_baseline_summaries[idx]\n","    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","    \n","    original_model_res = gen(original_model,prompt,100,)\n","    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n","    \n","    peft_model_res = gen(ft_model,prompt,100,)\n","    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n","    #print(peft_model_output)\n","    peft_model_text_output, success, result = peft_model_output.partition('#End')\n","    \n","\n","    original_model_summaries.append(original_model_text_output)\n","    peft_model_summaries.append(peft_model_text_output)\n","\n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n","df"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:36:07.268097Z","iopub.status.busy":"2024-05-16T10:36:07.267801Z","iopub.status.idle":"2024-05-16T10:36:09.079561Z","shell.execute_reply":"2024-05-16T10:36:09.078317Z","shell.execute_reply.started":"2024-05-16T10:36:07.268071Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e079e26ef4f64cee883288119d2e08bf","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ORIGINAL MODEL:\n","{'rouge1': 0.2808829786653489, 'rouge2': 0.10594067089744544, 'rougeL': 0.20164900056627, 'rougeLsum': 0.2125258194438865}\n","PEFT MODEL:\n","{'rouge1': 0.31440242150611264, 'rouge2': 0.12777019949969665, 'rougeL': 0.24896680231457047, 'rougeLsum': 0.2578809037890624}\n"]}],"source":["rouge = evaluate.load('rouge')\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_summaries,\n","    references=human_baseline_summaries[0:len(peft_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('PEFT MODEL:')\n","print(peft_model_results)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T10:36:09.083248Z","iopub.status.busy":"2024-05-16T10:36:09.082332Z","iopub.status.idle":"2024-05-16T10:36:09.090509Z","shell.execute_reply":"2024-05-16T10:36:09.089383Z","shell.execute_reply.started":"2024-05-16T10:36:09.083205Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n","rouge1: 3.35%\n","rouge2: 2.18%\n","rougeL: 4.73%\n","rougeLsum: 4.54%\n"]}],"source":["print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","\n","improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(peft_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Evaluation on Basic Prompts"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T11:05:53.305157Z","iopub.status.busy":"2024-05-16T11:05:53.304703Z","iopub.status.idle":"2024-05-16T11:05:55.886562Z","shell.execute_reply":"2024-05-16T11:05:55.885616Z","shell.execute_reply.started":"2024-05-16T11:05:53.305121Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Person1 and Person2 are at a party. Person1 compliments Person2 on their party and appearance. Person1 asks Person2 to dance. Person1 and Person2 have a drink together to celebrate.\n","\n"]}],"source":["dialogue = \"#Person1#: Happy Birthday, this is for you, Brian. Person2: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time. Person1: Brian, may I have a pleasure to have a dance with you? Person2: Ok. Person1: This is really wonderful party. Person2: Yes, you are always popular with everyone. and you look very pretty today. Person1: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel. Person2: You look great, you are absolutely glowing. Person1: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\"\n","prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","\n","original_model_res = gen(original_model,prompt,100,)\n","original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n","\n","print(original_model_text_output)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
