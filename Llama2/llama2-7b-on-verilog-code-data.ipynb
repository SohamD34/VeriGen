{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8622643,"sourceType":"datasetVersion","datasetId":5161838}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setting up Environment","metadata":{}},{"cell_type":"code","source":"!pip install cupy-cuda11x --quiet\nprint(\"Cuda ready for use\")\n\n!pip install accelerate@git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344 --quiet\nprint(\"Accelerate set up completed\")\n\n!pip install -q -U torch numpy shapely transformers peft datasets scipy einops evaluate trl\n!pip install bitsandbytes --quiet\nprint(\"Installed dependencies\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T15:50:19.802908Z","iopub.execute_input":"2024-07-19T15:50:19.803917Z","iopub.status.idle":"2024-07-19T15:54:18.097200Z","shell.execute_reply.started":"2024-07-19T15:50:19.803873Z","shell.execute_reply":"2024-07-19T15:54:18.096125Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCuda ready for use\nAccelerate set up completed\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mInstalled dependencies\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip freeze > requirements.txt\n\nwith open('/kaggle/working/requirements.txt') as f:\n    requirements = f.readlines()\n\nyaml_content = \"\"\"name: llmenv\nchannels:\n  - defaults\ndependencies:\n\"\"\"\nfor req in requirements:\n    yaml_content += f\"  - {req}\"\n\nwith open('environment.yaml', 'w') as f:\n    f.write(yaml_content)\n\nprint(\"environment.yaml file has been created.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T15:55:35.219386Z","iopub.execute_input":"2024-07-19T15:55:35.219794Z","iopub.status.idle":"2024-07-19T15:55:37.877113Z","shell.execute_reply.started":"2024-07-19T15:55:35.219740Z","shell.execute_reply":"2024-07-19T15:55:37.876059Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"environment.yaml file has been created.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    GenerationConfig,\n    set_seed,\n    StoppingCriteria, StoppingCriteriaList\n)\nimport os\nfrom pynvml import *\nimport accelerate\nimport bitsandbytes as bnb\nfrom datasets import load_dataset\nfrom functools import partial\nimport psutil\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport time\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-07-19T15:56:02.738300Z","iopub.execute_input":"2024-07-19T15:56:02.739178Z","iopub.status.idle":"2024-07-19T15:56:20.446216Z","shell.execute_reply.started":"2024-07-19T15:56:02.739138Z","shell.execute_reply":"2024-07-19T15:56:20.445266Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-19 15:56:09.228310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-19 15:56:09.228407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-19 15:56:09.352310: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_DISABLED']=\"true\"                 # disable Weights and Biases\n\ndef print_device():\n    if(torch.cuda.is_available()):\n        print('GPUs available =',torch.cuda.device_count())\n        for i in range(torch.cuda.device_count()):\n            print(torch.cuda.get_device_name(i))\n        device = 'cuda'\n    else:\n        device = 'cpu'\n    os.environ['device'] = device\n    print('Using primary device -',device)\n    \n\ndef print_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied by GPU:0 = {info.used//1024**2} MB.\")\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(1)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied by GPU:1 = {info.used//1024**2} MB.\")\n    \n    cpu_percent = psutil.cpu_percent(interval=1, percpu=True)\n    for i, cpu in enumerate(cpu_percent):\n        print(f\"CPU {i}: {cpu}%\", end=\" \")\n    print('\\n')\n    \n    \ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\n\ndef gen(model,p,tokenizer,maxlen=100,sample=True):\n    toks = tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return tokenizer.batch_decode(res,skip_special_tokens=True)\n\n\ndef truncate_at_stopwords(text, stopwords = ['\\endmodu']):\n    for stopword in stopwords:\n        stop_index = text.find(stopword)\n        if stop_index != -1:\n            return text[:stop_index]\n    return text\n\n\ndef pipeline(model, tokenizer, prompt):\n    dash_line = '-'.join('' for x in range(170))    \n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(os.environ['device'])\n    sample = model.generate(input_ids, pad_token_id=tokenizer.pad_token_id, max_length=250, temperature=0.2, top_p=0.9)\n    response = re.sub(r'\\n\\s*\\n', '\\n', tokenizer.decode(sample[0]))\n\n    print(response)\n    print(dash_line)\n    \n\ndef create_prompt_formats(df):\n    \n    blurb = \"BASE PROMPT: You are an expert in Verilog code generation and code correction. Below is an instruction that describes a task. Write a response that appropriately completes the request. Do not write any explanation after the code.\"\n    instruction = \"\\nInstruct: Correct the syntax and logic of following Verilog code.\"\n    end = \"End\"\n    \n    formatted_prompts =  []\n    for i in list(df[\"Error\"]):\n        formatted_prompts.append(\"\\n\\n\".join([part for part in [blurb, instruction, i, end] if part]))\n    df[\"Prompt\"] = formatted_prompts\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-19T15:56:42.196852Z","iopub.execute_input":"2024-07-19T15:56:42.198058Z","iopub.status.idle":"2024-07-19T15:56:42.214338Z","shell.execute_reply.started":"2024-07-19T15:56:42.198024Z","shell.execute_reply":"2024-07-19T15:56:42.213594Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Baseline model initialization","metadata":{}},{"cell_type":"code","source":"seed = 42\nset_seed(seed)\n\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\n\nprint_device()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T15:56:49.110105Z","iopub.execute_input":"2024-07-19T15:56:49.110773Z","iopub.status.idle":"2024-07-19T15:56:49.120128Z","shell.execute_reply.started":"2024-07-19T15:56:49.110739Z","shell.execute_reply":"2024-07-19T15:56:49.119256Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"GPUs available = 2\nTesla T4\nTesla T4\nUsing primary device - cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"silverliningeda/llama-2-7b-silverliningeda-verilog-codegen\"\n\nprint(\"Before tokenizer installation\")\ntorch.cuda.empty_cache()\nprint_utilization()\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map={\"\":0})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"                                                                            # Fix weird overflow issue\n\nprint(\"After tokenizer installation\")\nprint_utilization()\n\nprint(\"Before model installation\")\nprint_utilization()\n\nbase_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, quantization_config=bnb_config, device_map={\"\":1})\n\nprint(\"After model installation\")\nprint_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T15:57:04.154634Z","iopub.execute_input":"2024-07-19T15:57:04.155013Z","iopub.status.idle":"2024-07-19T16:02:57.527968Z","shell.execute_reply.started":"2024-07-19T15:57:04.154985Z","shell.execute_reply":"2024-07-19T16:02:57.526956Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Before tokenizer installation\nGPU memory occupied by GPU:0 = 267 MB.\nGPU memory occupied by GPU:1 = 267 MB.\nCPU 0: 0.0% CPU 1: 1.0% CPU 2: 1.0% CPU 3: 1.0% \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744a32784ed042178493cafb9adbf074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edeec4c57a704420b4cf02d8859c9e60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f5854909d1143eeae46053f732c27c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f62eacc74245cf9276ee7dd8e4c5d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec4264f86e94d2ca6d7d23963fbf0c7"}},"metadata":{}},{"name":"stdout","text":"After tokenizer installation\nGPU memory occupied by GPU:0 = 267 MB.\nGPU memory occupied by GPU:1 = 267 MB.\nCPU 0: 4.0% CPU 1: 4.0% CPU 2: 14.0% CPU 3: 17.8% \n\nBefore model installation\nGPU memory occupied by GPU:0 = 267 MB.\nGPU memory occupied by GPU:1 = 267 MB.\nCPU 0: 2.0% CPU 1: 3.0% CPU 2: 63.0% CPU 3: 6.9% \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df27923229994be6ac47c0ca5545dd59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e22b65f539be47f18865dfb9d963f722"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde855b9f14b4e90b10b34a6092f8352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fcbffe3218c463f911895c632dec7a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607fdb2b22f3426c85f7a4398cfbb962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20ca5a106364046bbcfcdf42dabbc75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e49b6b88be05461aa39cbd1aec5ca45e"}},"metadata":{}},{"name":"stdout","text":"After model installation\nGPU memory occupied by GPU:0 = 267 MB.\nGPU memory occupied by GPU:1 = 4565 MB.\nCPU 0: 2.0% CPU 1: 2.0% CPU 2: 2.0% CPU 3: 4.0% \n\n","output_type":"stream"}]},{"cell_type":"code","source":"blurb = \"\\nBASE PROMPT: You are an expert in Verilog code generation and code correction. Below is an instruction that describes a task. Write a response that appropriately completes the request. Do not write any explanation after the code.\"\nend = \"CODE:\\n\"\n\ninstruction = \"\\nINSTRUCT: Write a Verilog module for 1-bit half adder.\"\nformatted_prompt = \"\\n\\n\".join([part for part in [blurb, instruction, end] if part])\npipeline(base_model, tokenizer, formatted_prompt)\n\n# print(dash_line)\n\ninstruction = \"\\nINSTRUCT: Write a Verilog module and logic for 32-bit full adder.\"\nformatted_prompt = \"\\n\\n\".join([part for part in [blurb, instruction, end] if part])\npipeline(base_model, tokenizer, formatted_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:07:43.342382Z","iopub.execute_input":"2024-07-19T16:07:43.342757Z","iopub.status.idle":"2024-07-19T16:08:04.751068Z","shell.execute_reply.started":"2024-07-19T16:07:43.342728Z","shell.execute_reply":"2024-07-19T16:08:04.750144Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<s> \nBASE PROMPT: You are an expert in Verilog code generation and code correction. Below is an instruction that describes a task. Write a response that appropriately completes the request. Do not write any explanation after the code.\nINSTRUCT: Write a Verilog module for 1-bit half adder.\nCODE:\nmodule 1bit_half_adder (\noutput logic [7:0] sum,\noutput logic carry,\ninput logic [7:0] a,\ninput logic [7:0] b,\ninput logic cin\n);\nendmodule\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n<s> \nBASE PROMPT: You are an expert in Verilog code generation and code correction. Below is an instruction that describes a task. Write a response that appropriately completes the request. Do not write any explanation after the code.\nINSTRUCT: Write a Verilog module and logic for 32-bit full adder.\nCODE:\nmodule 32bit_adder (\noutput logic [31:0] sum,\noutput logic carry,\ninput logic [31:0] a,\ninput logic [31:0] b,\ninput logic cin\n);\nlogic [31:0] sum_temp;\nlogic carry_temp;\nassign {carry_temp, sum_temp} = a + b + cin;\nassign sum = sum_temp;\nassign carry = carry_temp;\nendmodule\nENDINST\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/llm-design-data/df_small.csv')\nprint(\"ORIGINAL DATASET\")\ndataset.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:09:43.464818Z","iopub.execute_input":"2024-07-19T16:09:43.465184Z","iopub.status.idle":"2024-07-19T16:09:44.137137Z","shell.execute_reply.started":"2024-07-19T16:09:43.465155Z","shell.execute_reply":"2024-07-19T16:09:44.136032Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                             Correct  \\\n0  /*\\n * Copyright 2012, Homer Hsing <homer.hsin...   \n1  // Two modules are built as part of solution\\n...   \n\n                                               Error  \n0  /*\\n  * Copyright 2012, Homer Hsing <homer.hsi...  \n1  // Two modules are built as part of solution\\n...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Correct</th>\n      <th>Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/*\\n * Copyright 2012, Homer Hsing &lt;homer.hsin...</td>\n      <td>/*\\n  * Copyright 2012, Homer Hsing &lt;homer.hsi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>// Two modules are built as part of solution\\n...</td>\n      <td>// Two modules are built as part of solution\\n...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset = create_prompt_formats(dataset)\ndataset.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:12:53.987320Z","iopub.execute_input":"2024-07-19T16:12:53.988432Z","iopub.status.idle":"2024-07-19T16:12:54.012321Z","shell.execute_reply.started":"2024-07-19T16:12:53.988398Z","shell.execute_reply":"2024-07-19T16:12:54.011276Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             Correct  \\\n0  /*\\n * Copyright 2012, Homer Hsing <homer.hsin...   \n1  // Two modules are built as part of solution\\n...   \n\n                                               Error  \\\n0  /*\\n  * Copyright 2012, Homer Hsing <homer.hsi...   \n1  // Two modules are built as part of solution\\n...   \n\n                                              Prompt  \n0  BASE PROMPT: You are an expert in Verilog code...  \n1  BASE PROMPT: You are an expert in Verilog code...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Correct</th>\n      <th>Error</th>\n      <th>Prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/*\\n * Copyright 2012, Homer Hsing &lt;homer.hsin...</td>\n      <td>/*\\n  * Copyright 2012, Homer Hsing &lt;homer.hsi...</td>\n      <td>BASE PROMPT: You are an expert in Verilog code...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>// Two modules are built as part of solution\\n...</td>\n      <td>// Two modules are built as part of solution\\n...</td>\n      <td>BASE PROMPT: You are an expert in Verilog code...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(base_model),\"\\n\")\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05, \n    task_type=\"CAUSAL_LM\",\n)\n\nbase_model.gradient_checkpointing_enable()\noriginal_model = prepare_model_for_kbit_training(base_model)\npeft_model = get_peft_model(base_model, config)\n\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:14:12.971480Z","iopub.execute_input":"2024-07-19T14:14:12.971895Z","iopub.status.idle":"2024-07-19T14:14:13.429631Z","shell.execute_reply.started":"2024-07-19T14:14:12.971863Z","shell.execute_reply":"2024-07-19T14:14:13.428508Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"trainable model parameters: 25165824\nall model parameters: 3525578752\npercentage of trainable model parameters: 0.71% \n\ntrainable model parameters: 25165824\nall model parameters: 3525578752\npercentage of trainable model parameters: 0.71%\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(peft_training_args.device)\npeft_trainer.train()\nprint_gpu_utilization()","metadata":{},"execution_count":null,"outputs":[]}]}